import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
doc = "Text analytics is the process of extracting meaningful information from text."

tokens = word_tokenize(doc)
print("Tokens:", tokens)
pos_tags = pos_tag(tokens)
print("POS Tag:",pos_tags)
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print("After Stop Words Removal:",filtered_tokens)
stemmer = PorterStemmer()
stemmed = [stemmer.stem(word) for word in filtered_tokens]
print("After Stemming:",stemmed)
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("After Lemmatization:",lemmatized)
documents = [
    "Text analytics is the process of extracting meaningful information from text.",
    "Analytics tools help extract useful insights from large text datasets."
]
tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(documents)
print("\nTF-IDF Matrix:")
print(tfidf_matrix.toarray())
print("\nFeature Names (Terms):")
print(tfidf.get_feature_names_out())
